{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2383a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "from simclr import SimCLR\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from flash.core.optimizers import LARS\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules import NT_Xent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01080481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "batch_size = 128\n",
    "\n",
    "# To make it work in both Jupyter and standalone:\n",
    "if \"__file__\" in globals():\n",
    "    root = pathlib.Path(__file__).parent.resolve()\n",
    "else:\n",
    "    # Probably running interactively; in Jupyter, notebook path is\n",
    "    # typically 'os.getcwd()', if it's not that's where we are going\n",
    "    # to store the CIFAR data.\n",
    "    import os\n",
    "    root = pathlib.Path(os.getcwd())\n",
    "    \n",
    "    \n",
    "dataset = CIFAR10(root=root, download=True, transform = TransformsSimCLR(size = image_size))\n",
    "torch.manual_seed(43)\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=False,\n",
    "                          drop_last = True,\n",
    "                          num_workers=2,\n",
    "                          sampler = None)\n",
    "\n",
    "train_dataset = CIFAR10(root = root, transform = transforms.ToTensor())\n",
    "\n",
    "loader = DataLoader(train_dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=False,\n",
    "                          drop_last = True,\n",
    "                          num_workers=2,\n",
    "                          sampler = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad800d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Shulu/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "epochs = 50\n",
    "\n",
    "encoder = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False) \n",
    "projection_dim = 64\n",
    "n_features = encoder.fc.in_features  # get dimensions of last fully-connected layer\n",
    "model = SimCLR(encoder, projection_dim, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360efcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LARS(model.parameters(), lr = 0.075 * np.sqrt(batch_size), weight_decay = 1e-6)\n",
    "criterion = NT_Xent(batch_size, temperature = 0.2, world_size=1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                       epochs, \n",
    "                                                       eta_min=0, \n",
    "                                                       last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d43d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/390]\t Loss: 2.4335145950317383\n",
      "Step [50/390]\t Loss: 1.1410763263702393\n",
      "Step [100/390]\t Loss: 1.1400648355484009\n",
      "Step [150/390]\t Loss: 1.1286624670028687\n",
      "Step [200/390]\t Loss: 1.1264699697494507\n",
      "Step [250/390]\t Loss: 1.121312141418457\n",
      "Step [300/390]\t Loss: 1.1232011318206787\n",
      "Step [350/390]\t Loss: 1.1196770668029785\n"
     ]
    }
   ],
   "source": [
    "def train(global_step, loader, model, criterion, optimizer, writer):\n",
    "    loss_epoch = 0\n",
    "    for steps, ((i, _)) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        #i = i.cuda(non_blocking = True)\n",
    "        h_i, h_j, z_i, z_j = model(i, i)\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % 50 == 0:\n",
    "            print(f\"Step [{steps}/{len(loader)}]\\t Loss: {loss.item()}\")\n",
    "\n",
    "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), global_step)\n",
    "        loss_epoch += loss.item()\n",
    "        global_step += 1\n",
    "    return loss_epoch\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = train(global_step, loader, model, criterion, optimizer, writer)\n",
    "    scheduler.step()\n",
    "    writer.add_scalar(\"Loss/train\", loss_epoch / len(loader), epoch)\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch = 0\n",
    "\n",
    "#the transformed dataset, my PC gets stuck inside the for-statement\n",
    "#Currently working with the for-loop above for debugging and testing purposes //Mohamad\n",
    "for step, ((x_i, x_j), _) in enumerate(train_loader): #program gets stuck exactly here\n",
    "    optimizer.zero_grad()\n",
    "    x_i = x_i.cuda(non_blocking=True)\n",
    "    x_j = x_j.cuda(non_blocking=True)\n",
    "\n",
    "    # positive pair, with encoding\n",
    "    h_i, h_j, z_i, z_j = model(x_i, x_j)\n",
    "\n",
    "    loss = criterion(z_i, z_j)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
